{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470d2962",
   "metadata": {},
   "source": [
    "#  Reinforcement Learning Assignment 2 – Taxi-v3 Environment  \n",
    "CSCN 8020 – Reinforcement Learning Programming  \n",
    "\n",
    "## Done by ***Eris Leksi***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336565ee",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76962e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & setup\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from IPython.display import clear_output, display\n",
    "import assignment2_utils as utils   # professor's helper file \n",
    "\n",
    "# Reproducibility & results folder\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81949565",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Import standard libraries, the professor's `assignment2_utils.py` helper module, set random seed, and create a results directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5247e6",
   "metadata": {},
   "source": [
    "## Create and Describe the Taxi Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c2be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(500)\n",
      "Number of observations: 500\n",
      "Action space: Discrete(6)\n",
      "Number of actions: 6\n",
      "Reward range: (-10, 20)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import importlib\n",
    "import assignment2_utils as utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "num_obs, num_actions = utils.describe_env(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd90dff",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "We create the `Taxi-v3` environment and call `describe_env()` from the supplied utility file to print action/observation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d72df5",
   "metadata": {},
   "source": [
    "## Optional: Quick random rollout to verify environment (no learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c2c2e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: action=0, reward=-1, done=False\n",
      "Step 2: action=4, reward=-10, done=False\n",
      "Step 3: action=2, reward=-1, done=False\n",
      "Step 4: action=0, reward=-1, done=False\n",
      "Step 5: action=0, reward=-1, done=False\n",
      "Step 6: action=1, reward=-1, done=False\n",
      "Step 7: action=1, reward=-1, done=False\n",
      "Step 8: action=5, reward=-10, done=False\n",
      "Step 9: action=0, reward=-1, done=False\n",
      "Step 10: action=4, reward=-10, done=False\n"
     ]
    }
   ],
   "source": [
    "# quick random rollout (short) to check env mechanics; no rendering here so it works in notebook\n",
    "state, _ = env.reset(seed=SEED)\n",
    "for step in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Step {step+1}: action={action}, reward={reward}, done={terminated or truncated}\")\n",
    "    state = next_state\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81cf65",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "A short random run to verify environment interactions (state transitions and rewards). This does not render a GUI and is safe to run in notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f350c6a",
   "metadata": {},
   "source": [
    "## Hyperparameters & Experiment Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f8982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline hyperparameters (assignment)\n",
    "ALPHA_BASE = 0.1\n",
    "EPSILON_BASE = 0.1\n",
    "GAMMA_BASE = 0.9\n",
    "\n",
    "# Parameter variations requested by the assignment\n",
    "ALPHA_VARIATIONS = [0.01, 0.001, 0.2]   # change alpha separately\n",
    "EPSILON_VARIATIONS = [0.2, 0.3]         # change epsilon separately (assignment probably meant ε)\n",
    "\n",
    "# Training settings\n",
    "N_EPISODES = 5000         \n",
    "MAX_STEPS_PER_EP = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0689c",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Define baseline hyperparameters, the specified variations to test, and training run-length parameters. Use `N_EPISODES`=5000 unless you want a faster debug run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0dff0",
   "metadata": {},
   "source": [
    "## Q-Learning Training Function (tabular)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c49155a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, n_episodes, alpha, gamma, epsilon, max_steps_per_episode=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Tabular Q-Learning on Taxi-v3.\n",
    "    Returns: dict with episode list, steps, returns, final Q-table, runtime seconds.\n",
    "    \"\"\"\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    Q = np.zeros((n_states, n_actions), dtype=float)\n",
    "\n",
    "    episode_steps = []\n",
    "    episode_returns = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for ep in range(1, n_episodes + 1):\n",
    "        reset_out = env.reset(seed=SEED + ep)  # slight variation to seed each episode\n",
    "        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
    "\n",
    "        ep_return = 0\n",
    "        ep_steps = 0\n",
    "        for _ in range(max_steps_per_episode):\n",
    "            # epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = int(np.argmax(Q[state]))\n",
    "\n",
    "            step_out = env.step(action)\n",
    "            # gymnasium returns (obs, reward, terminated, truncated, info)\n",
    "            if len(step_out) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = step_out\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = step_out\n",
    "\n",
    "            # Q-learning update\n",
    "            best_next = np.max(Q[next_state])\n",
    "            td_target = reward + gamma * best_next\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "            ep_return += reward\n",
    "            ep_steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_steps.append(ep_steps)\n",
    "        episode_returns.append(ep_return)\n",
    "\n",
    "        if verbose and (ep % 500 == 0 or ep == 1):\n",
    "            print(f\"Ep {ep}/{n_episodes} avg_last100={np.mean(episode_returns[-100:]):.2f}\")\n",
    "\n",
    "    runtime = time.time() - t0\n",
    "    return {\n",
    "        \"episode\": list(range(1, n_episodes + 1)),\n",
    "        \"steps\": episode_steps,\n",
    "        \"returns\": episode_returns,\n",
    "        \"Q\": Q,\n",
    "        \"runtime_sec\": runtime\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a2835",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This is the tabular Q-Learning training function using the ε-greedy policy. It returns per-episode steps and returns, plus the final Q-table and runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146d80b",
   "metadata": {},
   "source": [
    "## Helper: Save metrics and plot utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3631864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_csv(metrics, label):\n",
    "    df = pd.DataFrame({\"episode\": metrics[\"episode\"], \"steps\": metrics[\"steps\"], \"return\": metrics[\"returns\"]})\n",
    "    path = os.path.join(RESULTS_DIR, f\"metrics_{label}.csv\")\n",
    "    df.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "def plot_metrics(metrics, label, show=True):\n",
    "    episodes = metrics[\"episode\"]\n",
    "    steps = metrics[\"steps\"]\n",
    "    returns = metrics[\"returns\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(9, 7), sharex=True)\n",
    "    axes[0].plot(episodes, steps)\n",
    "    axes[0].set_ylabel(\"Steps per episode\")\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    rolling = pd.Series(returns).rolling(100, min_periods=1).mean()\n",
    "    axes[1].plot(episodes, returns, alpha=0.3, label=\"return\")\n",
    "    axes[1].plot(episodes, rolling, label=\"rolling mean (100)\", linewidth=2)\n",
    "    axes[1].set_ylabel(\"Return\")\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    plt.suptitle(f\"Q-Learning: {label}\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    png_path = os.path.join(RESULTS_DIR, f\"plot_{label}.png\")\n",
    "    plt.savefig(png_path)\n",
    "    if show:\n",
    "        display(fig)\n",
    "    plt.close(fig)\n",
    "    return png_path\n",
    "\n",
    "def summarize_metrics(metrics):\n",
    "    ep_count = len(metrics[\"episode\"])\n",
    "    return {\n",
    "        \"episodes\": ep_count,\n",
    "        \"avg_steps\": float(np.mean(metrics[\"steps\"])),\n",
    "        \"avg_return\": float(np.mean(metrics[\"returns\"])),\n",
    "        \"last100_avg_return\": float(np.mean(metrics[\"returns\"][-100:])),\n",
    "        \"total_steps\": int(np.sum(metrics[\"steps\"])),\n",
    "        \"runtime_sec\": float(metrics.get(\"runtime_sec\", 0.0))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ff6b9",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Utility functions to save CSVs, plot learning curves, and summarize run statistics for reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d459f",
   "metadata": {},
   "source": [
    "## Run Baseline & Hyperparameter Experiments (Q-Learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485348e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline...\n",
      "Running alpha variation: alpha_0.01\n",
      "Running alpha variation: alpha_0.001\n",
      "Running alpha variation: alpha_0.2\n",
      "Running epsilon variation: epsilon_0.2\n",
      "Running epsilon variation: epsilon_0.3\n",
      "All experiments finished.\n"
     ]
    }
   ],
   "source": [
    "# Run baseline\n",
    "experiments = []\n",
    "\n",
    "print(\"Running baseline...\")\n",
    "metrics_base = train_q_learning(env, N_EPISODES, ALPHA_BASE, GAMMA_BASE, EPSILON_BASE, MAX_STEPS_PER_EP)\n",
    "experiments.append((\"baseline\", ALPHA_BASE, EPSILON_BASE, GAMMA_BASE, metrics_base))\n",
    "save_metrics_csv(metrics_base, \"baseline\")\n",
    "plot_metrics(metrics_base, \"baseline\", show=False)\n",
    "\n",
    "# Run alpha variations (keep epsilon, gamma baseline)\n",
    "for a in ALPHA_VARIATIONS:\n",
    "    label = f\"alpha_{a}\"\n",
    "    print(f\"Running alpha variation: {label}\")\n",
    "    m = train_q_learning(env, N_EPISODES, a, GAMMA_BASE, EPSILON_BASE, MAX_STEPS_PER_EP)\n",
    "    experiments.append((label, a, EPSILON_BASE, GAMMA_BASE, m))\n",
    "    save_metrics_csv(m, label)\n",
    "    plot_metrics(m, label, show=False)\n",
    "\n",
    "# Run epsilon variations (keep alpha baseline)\n",
    "for e in EPSILON_VARIATIONS:\n",
    "    label = f\"epsilon_{e}\"\n",
    "    print(f\"Running epsilon variation: {label}\")\n",
    "    m = train_q_learning(env, N_EPISODES, ALPHA_BASE, GAMMA_BASE, e, MAX_STEPS_PER_EP)\n",
    "    experiments.append((label, ALPHA_BASE, e, GAMMA_BASE, m))\n",
    "    save_metrics_csv(m, label)\n",
    "    plot_metrics(m, label, show=False)\n",
    "\n",
    "print(\"All experiments finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333252cb",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Run baseline training and all requested hyperparameter variations separately. Each run saves metrics and a plot PNG into the `results/` folder. This cell is the main deliverable code for the “Python code implementing Q-Learning and running it for the different hyperparameters.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947786bb",
   "metadata": {},
   "source": [
    "## Compare Experiments and Choose Best Combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef06ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "alpha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "epsilon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gamma",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "episodes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "avg_steps",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "avg_return",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "last100_avg_return",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_steps",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "runtime_sec",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ad4e1b19-6843-49b6-a799-90f7811b6e99",
       "rows": [
        [
         "0",
         "baseline",
         "0.1",
         "0.1",
         "0.9",
         "5000",
         "30.2672",
         "-21.557",
         "2.3",
         "151336",
         "10.037344694137573"
        ],
        [
         "1",
         "alpha_0.2",
         "0.2",
         "0.1",
         "0.9",
         "5000",
         "23.4146",
         "-11.519",
         "1.82",
         "117073",
         "7.228285789489746"
        ],
        [
         "2",
         "epsilon_0.2",
         "0.1",
         "0.2",
         "0.9",
         "5000",
         "32.7438",
         "-32.8638",
         "-4.75",
         "163719",
         "10.830549478530884"
        ],
        [
         "3",
         "epsilon_0.3",
         "0.1",
         "0.3",
         "0.9",
         "5000",
         "36.0178",
         "-47.371",
         "-15.03",
         "180089",
         "11.740064144134521"
        ],
        [
         "4",
         "alpha_0.01",
         "0.01",
         "0.1",
         "0.9",
         "5000",
         "126.416",
         "-159.6092",
         "-66.79",
         "632080",
         "33.90319871902466"
        ],
        [
         "5",
         "alpha_0.001",
         "0.001",
         "0.1",
         "0.9",
         "5000",
         "185.3298",
         "-258.504",
         "-246.82",
         "926649",
         "51.99786853790283"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>gamma</th>\n",
       "      <th>episodes</th>\n",
       "      <th>avg_steps</th>\n",
       "      <th>avg_return</th>\n",
       "      <th>last100_avg_return</th>\n",
       "      <th>total_steps</th>\n",
       "      <th>runtime_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>30.2672</td>\n",
       "      <td>-21.5570</td>\n",
       "      <td>2.30</td>\n",
       "      <td>151336</td>\n",
       "      <td>10.037345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alpha_0.2</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>23.4146</td>\n",
       "      <td>-11.5190</td>\n",
       "      <td>1.82</td>\n",
       "      <td>117073</td>\n",
       "      <td>7.228286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>epsilon_0.2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>32.7438</td>\n",
       "      <td>-32.8638</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>163719</td>\n",
       "      <td>10.830549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>epsilon_0.3</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>36.0178</td>\n",
       "      <td>-47.3710</td>\n",
       "      <td>-15.03</td>\n",
       "      <td>180089</td>\n",
       "      <td>11.740064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alpha_0.01</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>126.4160</td>\n",
       "      <td>-159.6092</td>\n",
       "      <td>-66.79</td>\n",
       "      <td>632080</td>\n",
       "      <td>33.903199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alpha_0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5000</td>\n",
       "      <td>185.3298</td>\n",
       "      <td>-258.5040</td>\n",
       "      <td>-246.82</td>\n",
       "      <td>926649</td>\n",
       "      <td>51.997869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label  alpha  epsilon  gamma  episodes  avg_steps  avg_return  \\\n",
       "0     baseline  0.100      0.1    0.9      5000    30.2672    -21.5570   \n",
       "1    alpha_0.2  0.200      0.1    0.9      5000    23.4146    -11.5190   \n",
       "2  epsilon_0.2  0.100      0.2    0.9      5000    32.7438    -32.8638   \n",
       "3  epsilon_0.3  0.100      0.3    0.9      5000    36.0178    -47.3710   \n",
       "4   alpha_0.01  0.010      0.1    0.9      5000   126.4160   -159.6092   \n",
       "5  alpha_0.001  0.001      0.1    0.9      5000   185.3298   -258.5040   \n",
       "\n",
       "   last100_avg_return  total_steps  runtime_sec  \n",
       "0                2.30       151336    10.037345  \n",
       "1                1.82       117073     7.228286  \n",
       "2               -4.75       163719    10.830549  \n",
       "3              -15.03       180089    11.740064  \n",
       "4              -66.79       632080    33.903199  \n",
       "5             -246.82       926649    51.997869  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build summary table of experiments\n",
    "rows = []\n",
    "for label, alpha, eps, gamma, metrics in experiments:\n",
    "    s = summarize_metrics(metrics)\n",
    "    rows.append({\n",
    "        \"label\": label, \"alpha\": alpha, \"epsilon\": eps, \"gamma\": gamma,\n",
    "        \"episodes\": s[\"episodes\"], \"avg_steps\": s[\"avg_steps\"],\n",
    "        \"avg_return\": s[\"avg_return\"], \"last100_avg_return\": s[\"last100_avg_return\"],\n",
    "        \"total_steps\": s[\"total_steps\"], \"runtime_sec\": s[\"runtime_sec\"]\n",
    "    })\n",
    "df_summary = pd.DataFrame(rows).sort_values(by=\"last100_avg_return\", ascending=False).reset_index(drop=True)\n",
    "display(df_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccec618",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Summarize metrics (average return, last-100 average, steps) and show a comparison table. Use the `last100_avg_return` column to pick the best-performing hyperparameter combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f0849",
   "metadata": {},
   "source": [
    "## Re-run the Best Combination and Save Q-Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9031f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found: baseline (alpha=0.1, eps=0.1) — re-running to confirm.\n",
      "Ep 1/5000 avg_last100=-569.00\n",
      "Ep 500/5000 avg_last100=-96.71\n",
      "Ep 1000/5000 avg_last100=-14.82\n",
      "Ep 1500/5000 avg_last100=-2.74\n",
      "Ep 2000/5000 avg_last100=-0.60\n",
      "Ep 2500/5000 avg_last100=1.65\n",
      "Ep 3000/5000 avg_last100=2.30\n",
      "Ep 3500/5000 avg_last100=2.14\n",
      "Ep 4000/5000 avg_last100=2.72\n",
      "Ep 4500/5000 avg_last100=3.37\n",
      "Ep 5000/5000 avg_last100=2.12\n",
      "Saved Q-table to: results\\q_table_baseline.npy\n"
     ]
    }
   ],
   "source": [
    "# pick best by last100_avg_return\n",
    "best_row = df_summary.iloc[0]\n",
    "best_label = best_row[\"label\"]\n",
    "best_alpha = best_row[\"alpha\"]\n",
    "best_eps = best_row[\"epsilon\"]\n",
    "best_gamma = best_row[\"gamma\"]\n",
    "\n",
    "print(f\"Best found: {best_label} (alpha={best_alpha}, eps={best_eps}) — re-running to confirm.\")\n",
    "metrics_best = train_q_learning(env, N_EPISODES, best_alpha, best_gamma, best_eps, MAX_STEPS_PER_EP, verbose=True)\n",
    "experiments.append((\"best_rerun\", best_alpha, best_eps, best_gamma, metrics_best))\n",
    "save_metrics_csv(metrics_best, \"best_rerun\")\n",
    "plot_metrics(metrics_best, \"best_rerun\", show=False)\n",
    "\n",
    "# save Q-table\n",
    "q_table_path = os.path.join(RESULTS_DIR, f\"q_table_{best_label}.npy\")\n",
    "np.save(q_table_path, metrics_best[\"Q\"])\n",
    "print(\"Saved Q-table to:\", q_table_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8886b",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Re-run training with the best hyperparameters (from comparisons) to confirm results and save the learned Q-table (NumPy `.npy`) for later evaluation/simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2c0b1",
   "metadata": {},
   "source": [
    "## Generate a PDF Report (metrics, plots, short comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dad94196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report saved at: results\\assignment2_report.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create a simple multipage PDF with summary and plots\n",
    "pdf_path = os.path.join(RESULTS_DIR, \"assignment2_report.pdf\")\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    # Title page\n",
    "    fig = plt.figure(figsize=(8.5, 11))\n",
    "    fig.text(0.5, 0.85, \"CSCN 8020 — Assignment 2: Q-Learning on Taxi-v3\", ha=\"center\", fontsize=16)\n",
    "    fig.text(0.5, 0.80, time.strftime(\"%Y-%m-%d %H:%M:%S\"), ha=\"center\", fontsize=10)\n",
    "    fig.text(0.1, 0.7, \"Deliverables included:\", fontsize=12)\n",
    "    fig.text(0.12, 0.66, \"- Python code implementing Q-Learning and hyperparameter runs (notebook).\")\n",
    "    fig.text(0.12, 0.62, \"- Metrics CSVs and plots in results/\")\n",
    "    fig.text(0.12, 0.58, \"- Q-table for best run.\")\n",
    "    pdf.savefig()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Summary table page\n",
    "    fig = plt.figure(figsize=(11, 8.5))\n",
    "    fig.suptitle(\"Experiment Summary (sorted by last 100 avg return)\", fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "    tbl = plt.table(cellText=df_summary.round(3).values, colLabels=df_summary.columns, loc=\"center\")\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(8)\n",
    "    tbl.scale(1, 1.5)\n",
    "    pdf.savefig()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Plot pages for each experiment (embed saved pngs)\n",
    "    for label, alpha, eps, gamma, metrics in experiments:\n",
    "        png = os.path.join(RESULTS_DIR, f\"plot_{label}.png\")\n",
    "        if os.path.exists(png):\n",
    "            img = plt.imread(png)\n",
    "            fig = plt.figure(figsize=(8.5, 11))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            pdf.savefig()\n",
    "            plt.close(fig)\n",
    "\n",
    "print(\"PDF report saved at:\", pdf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379cfcb",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Generate `assignment2_report.pdf` containing a title page, a comparison table, and the saved learning-curve plots for each experiment. This satisfies the PDF deliverable requirement with metrics and plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e5046",
   "metadata": {},
   "source": [
    "## Agent Wrapper for Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955b292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    \"\"\"\n",
    "    Simple wrapper that exposes select_action(state) as required by utils.simulate_episodes().\n",
    "    Uses an internal Q-table; selects greedy actions by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q_table):\n",
    "        self.Q = Q_table\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # greedy selection from Q-table (used for simulation)\n",
    "        return int(np.argmax(self.Q[state]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13cd4c",
   "metadata": {},
   "source": [
    "## Simulate the trained agent (visual, human render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fef0fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Episode 1 ---\n",
      "Passenger is at: Yellow, wants to go to Blue. Taxi currently at (2, 0)\n",
      "Episode 1 finished with total reward: 10\n",
      "\n",
      "--- Episode 2 ---\n",
      "Passenger is at: Yellow, wants to go to Blue. Taxi currently at (4, 3)\n",
      "Episode 2 finished with total reward: 5\n",
      "\n",
      "--- Episode 3 ---\n",
      "Passenger is at: Green, wants to go to Yellow. Taxi currently at (4, 3)\n",
      "Episode 3 finished with total reward: 6\n",
      "\n",
      "--- Episode 4 ---\n",
      "Passenger is at: Yellow, wants to go to Green. Taxi currently at (4, 0)\n",
      "Episode 4 finished with total reward: 11\n",
      "\n",
      "--- Episode 5 ---\n",
      "Passenger is at: Green, wants to go to Blue. Taxi currently at (0, 4)\n",
      "Episode 5 finished with total reward: 14\n",
      "\n",
      "--- Episode 6 ---\n",
      "Passenger is at: Red, wants to go to Yellow. Taxi currently at (4, 3)\n",
      "Episode 6 finished with total reward: 8\n",
      "\n",
      "--- Episode 7 ---\n",
      "Passenger is at: Blue, wants to go to Green. Taxi currently at (4, 0)\n",
      "Episode 7 finished with total reward: 7\n",
      "\n",
      "--- Episode 8 ---\n",
      "Passenger is at: Red, wants to go to Blue. Taxi currently at (0, 4)\n",
      "Episode 8 finished with total reward: 4\n",
      "\n",
      "--- Episode 9 ---\n",
      "Passenger is at: Blue, wants to go to Green. Taxi currently at (4, 3)\n",
      "Episode 9 finished with total reward: 14\n",
      "\n",
      "--- Episode 10 ---\n",
      "Passenger is at: Yellow, wants to go to Green. Taxi currently at (0, 4)\n",
      "Episode 10 finished with total reward: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "# ---- Settings ----\n",
    "NUM_EPISODES_SIM = 10\n",
    "MAX_STEPS_PER_EP = 200    # keep same as training\n",
    "RENDER_DELAY = 0.05       # seconds between renders; adjust to taste (0.0 = fastest)\n",
    "\n",
    "# Names for taxi pickup/drop locations used by Taxi-v3 (standard)\n",
    "# index 0..3 correspond to the four fixed map locations in Taxi-v3\n",
    "LOC_NAMES = [\"Red\", \"Green\", \"Yellow\", \"Blue\"]\n",
    "\n",
    "# ---- Prepare agent and env ----\n",
    "agent = QAgent(metrics_best[\"Q\"])\n",
    "env_vis = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "\n",
    "# Start first episode (no seed -> allow randomness)\n",
    "state, info = env_vis.reset()        # initial state with random passenger/dest\n",
    "done = False\n",
    "\n",
    "for ep in range(1, NUM_EPISODES_SIM + 1):\n",
    "    # Decode to get readable components\n",
    "    taxi_row, taxi_col, passenger_loc, dest = env_vis.unwrapped.decode(state)\n",
    "\n",
    "    # If passenger_loc == 4, passenger is already in taxi — show that as \"in taxi\"\n",
    "    passenger_str = \"in taxi\" if passenger_loc == 4 else LOC_NAMES[passenger_loc]\n",
    "    dest_str = LOC_NAMES[dest]\n",
    "\n",
    "    # Print clear start-of-episode summary (taxi coords shown as (row, col))\n",
    "    print(f\"--- Episode {ep} ---\")\n",
    "    print(f\"Passenger is at: {passenger_str}, wants to go to {dest_str}. Taxi currently at ({taxi_row:.0f}, {taxi_col:.0f})\")\n",
    "\n",
    "    # Run episode until termination (uses greedy policy)\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "    while not done and step < MAX_STEPS_PER_EP:\n",
    "        action = agent.select_action(state)\n",
    "        step_out = env_vis.step(action)\n",
    "        # gymnasium returns either 5-tuple or 4-tuple depending on version\n",
    "        if len(step_out) == 5:\n",
    "            next_state, reward, terminated, truncated, info = step_out\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_state, reward, done, info = step_out\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "        # render human window (will show visually)\n",
    "        try:\n",
    "            env_vis.render()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # small delay so you can watch; set to 0 for faster execution\n",
    "        time.sleep(RENDER_DELAY)\n",
    "\n",
    "    print(f\"Episode {ep} finished with total reward: {total_reward}\\n\")\n",
    "\n",
    "    # Prepare next episode (if any): keep taxi at current location,\n",
    "    # but resample passenger & destination randomly (ensure passenger != dest)\n",
    "    # We use the environment's RNG so results differ each time.\n",
    "    if ep < NUM_EPISODES_SIM:\n",
    "        taxi_row, taxi_col, _, _ = env_vis.unwrapped.decode(state)\n",
    "        # sample passenger_loc in 0..3 (on-map), and dest in 0..3, ensuring they differ\n",
    "        rng = env_vis.unwrapped.np_random\n",
    "        new_pass = int(rng.integers(0, 4))\n",
    "        new_dest = int(rng.integers(0, 4))\n",
    "        # ensure passenger != destination\n",
    "        while new_dest == new_pass:\n",
    "            new_dest = int(rng.integers(0, 4))\n",
    "\n",
    "        # encode state and set it without teleporting taxi coords (we already set taxi_row/col)\n",
    "        state = env_vis.unwrapped.encode(taxi_row, taxi_col, new_pass, new_dest)\n",
    "        env_vis.unwrapped.s = state\n",
    "        # small render to reflect new passenger on the map before next episode\n",
    "        try:\n",
    "            env_vis.render()\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(0.12)\n",
    "\n",
    "# Done\n",
    "env_vis.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29c32b",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This cell runs `simulate_episodes()` from the provided utils file with the trained greedy agent. It uses `render_mode=\"human\"` to visualize the taxi in action. Run this after training when you want to see the behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce78ac",
   "metadata": {},
   "source": [
    "## Experiment Summary\n",
    "\n",
    "- The **baseline** and each **parameter variation** were trained for **5,000 episodes**.  \n",
    "- Performance was evaluated using **average return** and **last-100-episode mean reward** to measure convergence trends.  \n",
    "- All detailed results, learning curves, and comparison tables are included in the accompanying **assignment2_report.pdf**.  \n",
    "- Supporting files — including CSV logs and trained Q-tables — are saved in the `results/` directory.  \n",
    "- Based on the final **last-100 average return**, the **baseline configuration (α=0.1, ε=0.1, γ=0.9)** performed best and was re-run for confirmation.  \n",
    "  The resulting Q-table was saved as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1e48c",
   "metadata": {},
   "source": [
    "- **Suggested Next Steps:**  \n",
    "- Implement **ε-decay** for dynamic exploration control.  \n",
    "- Extend training to **10,000 episodes** for improved convergence stability.  \n",
    "- Compare results against **SARSA** and **Deep Q-Network (DQN)** implementations for broader insight.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
